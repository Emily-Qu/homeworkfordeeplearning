{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac776f87-1c23-451f-9a1c-1dce22e7831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP model architecture:\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "CNN model architecture:\n",
      "CNN(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=125, out_features=30, bias=True)\n",
      "  (fc2): Linear(in_features=30, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 3)  # input channels: 1, output channels: 3, kernel size: 3\n",
    "        self.pool = nn.MaxPool2d(2, 2)   # kernel size: 2, stride: 2\n",
    "        self.conv2 = nn.Conv2d(3, 5, 3)  # input channels: 3, output channels: 5, kernel size: 3\n",
    "        self.fc1 = nn.Linear(5 * 5 * 5, 30)  # 5x5 image size after 3 max pooling layers\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 5 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create instances of MLP and CNN\n",
    "mlp_model = MLP()\n",
    "cnn_model = CNN()\n",
    "\n",
    "# Print the models\n",
    "print(\"MLP model architecture:\")\n",
    "print(mlp_model)\n",
    "\n",
    "print(\"\\nCNN model architecture:\")\n",
    "print(cnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5740efca-b2d0-4083-b66d-7d85520c85ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP...\n",
      "Epoch 1/5, Training Loss: 0.4847, Validation Loss: 0.0415, Validation Accuracy: 84.83%\n",
      "Epoch 2/5, Training Loss: 0.3513, Validation Loss: 0.0396, Validation Accuracy: 85.72%\n",
      "Epoch 3/5, Training Loss: 0.3168, Validation Loss: 0.0344, Validation Accuracy: 87.42%\n",
      "Epoch 4/5, Training Loss: 0.2943, Validation Loss: 0.0335, Validation Accuracy: 87.92%\n",
      "Epoch 5/5, Training Loss: 0.2786, Validation Loss: 0.0327, Validation Accuracy: 88.35%\n",
      "Testing MLP...\n",
      "Test Accuracy: 87.16%\n",
      "\n",
      "Training CNN...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [5, 3, 3, 3], expected input[64, 5, 5, 5] to have 3 channels, but got 5 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 138\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Train CNN\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining CNN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m train(cnn_model, train_loader, valid_loader, criterion, cnn_optimizer)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Test CNN\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting CNN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 81\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     80\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 81\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     82\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     83\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 68\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m     67\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m---> 68\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     70\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [5, 3, 3, 3], expected input[64, 5, 5, 5] to have 3 channels, but got 5 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_set = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# Split training set into training and validation\n",
    "num_train = len(train_set)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.1 * num_train))  # 10% of training set for validation\n",
    "np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, sampler=train_sampler)\n",
    "valid_loader = DataLoader(train_set, batch_size=64, sampler=valid_sampler)\n",
    "\n",
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 5, 3)\n",
    "        self.fc1 = nn.Linear(5 * 5 * 5, 30)\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 5 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define training function\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                validation_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Training Loss: {running_loss/len(train_loader.dataset):.4f}, \"\n",
    "              f\"Validation Loss: {validation_loss/len(valid_loader.dataset):.4f}, \"\n",
    "              f\"Validation Accuracy: {(100 * correct / total):.2f}%\")\n",
    "\n",
    "# Define evaluation function\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {(100 * correct / total):.2f}%\")\n",
    "\n",
    "# Create instances of MLP and CNN\n",
    "mlp_model = MLP()\n",
    "cnn_model = CNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train MLP\n",
    "print(\"Training MLP...\")\n",
    "train(mlp_model, train_loader, valid_loader, criterion, mlp_optimizer)\n",
    "\n",
    "# Test MLP\n",
    "print(\"Testing MLP...\")\n",
    "test(mlp_model, test_loader)\n",
    "\n",
    "# Train CNN\n",
    "print(\"\\nTraining CNN...\")\n",
    "train(cnn_model, train_loader, valid_loader, criterion, cnn_optimizer)\n",
    "\n",
    "# Test CNN\n",
    "print(\"Testing CNN...\")\n",
    "test(cnn_model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a780b06-ddb9-40e6-a769-49f9d0bd64b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP with lr=0.001 and batch_size=32...\n",
      "Epoch 1/5, Training Loss: 0.4852, Validation Loss: 0.0428, Validation Accuracy: 84.27%\n",
      "Epoch 2/5, Training Loss: 0.3524, Validation Loss: 0.0376, Validation Accuracy: 85.75%\n",
      "Epoch 3/5, Training Loss: 0.3197, Validation Loss: 0.0363, Validation Accuracy: 86.37%\n",
      "Epoch 4/5, Training Loss: 0.2952, Validation Loss: 0.0364, Validation Accuracy: 86.30%\n",
      "Epoch 5/5, Training Loss: 0.2767, Validation Loss: 0.0349, Validation Accuracy: 87.12%\n",
      "Testing MLP...\n",
      "Test Accuracy: 86.49%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m test(mlp_model, test_loader)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Update best hyperparameters if necessary\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n\u001b[1;32m     32\u001b[0m     best_accuracy \u001b[38;5;241m=\u001b[39m accuracy\n\u001b[1;32m     33\u001b[0m     best_lr \u001b[38;5;241m=\u001b[39m lr\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define a range of learning rates and batch sizes to explore\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_lr = None\n",
    "best_batch_size = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        # Create instances of MLP and CNN\n",
    "        mlp_model = MLP()\n",
    "        cnn_model = CNN()\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=lr)\n",
    "        cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=lr)\n",
    "\n",
    "        # Train MLP\n",
    "        print(f\"Training MLP with lr={lr} and batch_size={batch_size}...\")\n",
    "        train(mlp_model, train_loader, valid_loader, criterion, mlp_optimizer)\n",
    "\n",
    "        # Test MLP\n",
    "        print(\"Testing MLP...\")\n",
    "        accuracy = test(mlp_model, test_loader)\n",
    "        \n",
    "        # Update best hyperparameters if necessary\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_lr = lr\n",
    "            best_batch_size = batch_size\n",
    "\n",
    "        # Train CNN\n",
    "        print(f\"\\nTraining CNN with lr={lr} and batch_size={batch_size}...\")\n",
    "        train(cnn_model, train_loader, valid_loader, criterion, cnn_optimizer)\n",
    "\n",
    "        # Test CNN\n",
    "        print(\"Testing CNN...\")\n",
    "        accuracy = test(cnn_model, test_loader)\n",
    "        \n",
    "        # Update best hyperparameters if necessary\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_lr = lr\n",
    "            best_batch_size = batch_size\n",
    "\n",
    "print(f\"Best accuracy: {best_accuracy:.2f}%\")\n",
    "print(f\"Best learning rate: {best_lr}\")\n",
    "print(f\"Best batch size: {best_batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e829b214-caf0-4ab8-9f4a-a22cf34f0f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP...\n",
      "Epoch 1/5, Training Loss: 0.4847, Validation Loss: 0.0415, Validation Accuracy: 84.83%\n",
      "Epoch 2/5, Training Loss: 0.3513, Validation Loss: 0.0396, Validation Accuracy: 85.72%\n",
      "Epoch 3/5, Training Loss: 0.3168, Validation Loss: 0.0344, Validation Accuracy: 87.42%\n",
      "Epoch 4/5, Training Loss: 0.2943, Validation Loss: 0.0335, Validation Accuracy: 87.92%\n",
      "Epoch 5/5, Training Loss: 0.2786, Validation Loss: 0.0327, Validation Accuracy: 88.35%\n",
      "Testing MLP...\n",
      "Test Accuracy: 87.16%\n",
      "\n",
      "Training CNN...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [5, 3, 3, 3], expected input[64, 5, 5, 5] to have 3 channels, but got 5 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Train CNN\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining CNN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m train(cnn_model, train_loader, valid_loader, criterion, cnn_optimizer)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Test CNN\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting CNN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 81\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     80\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 81\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     82\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     83\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 68\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m     67\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m---> 68\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     70\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [5, 3, 3, 3], expected input[64, 5, 5, 5] to have 3 channels, but got 5 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_set = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# Split training set into training and validation\n",
    "num_train = len(train_set)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.1 * num_train))  # 10% of training set for validation\n",
    "np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, sampler=train_sampler)\n",
    "valid_loader = DataLoader(train_set, batch_size=64, sampler=valid_sampler)\n",
    "\n",
    "# Define MLP model with ReLU activation\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define CNN model with ReLU activation\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 5, 3)\n",
    "        self.fc1 = nn.Linear(5 * 5 * 5, 30)\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 5 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define training function\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                validation_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Training Loss: {running_loss/len(train_loader.dataset):.4f}, \"\n",
    "              f\"Validation Loss: {validation_loss/len(valid_loader.dataset):.4f}, \"\n",
    "              f\"Validation Accuracy: {(100 * correct / total):.2f}%\")\n",
    "\n",
    "# Define evaluation function\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Create instances of MLP and CNN\n",
    "mlp_model = MLP()\n",
    "cnn_model = CNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train MLP\n",
    "print(\"Training MLP...\")\n",
    "train(mlp_model, train_loader, valid_loader, criterion, mlp_optimizer)\n",
    "\n",
    "# Test MLP\n",
    "print(\"Testing MLP...\")\n",
    "mlp_accuracy = test(mlp_model, test_loader)\n",
    "\n",
    "# Train CNN\n",
    "print(\"\\nTraining CNN...\")\n",
    "train(cnn_model, train_loader, valid_loader, criterion, cnn_optimizer)\n",
    "\n",
    "# Test CNN\n",
    "print(\"Testing CNN...\")\n",
    "cnn_accuracy = test(cnn_model, test_loader)\n",
    "\n",
    "print(f\"\\nMLP Test Accuracy: {mlp_accuracy:.2f}%\")\n",
    "print(f\"CNN Test Accuracy: {cnn_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f86e1d0-2803-42ad-892f-d17c8623acfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP...\n",
      "Epoch 1/5, Training Loss: 0.7967, Validation Loss: 0.0516, Validation Accuracy: 82.18%\n",
      "Epoch 2/5, Training Loss: 0.4056, Validation Loss: 0.0430, Validation Accuracy: 84.97%\n",
      "Epoch 3/5, Training Loss: 0.3503, Validation Loss: 0.0378, Validation Accuracy: 86.42%\n",
      "Epoch 4/5, Training Loss: 0.3200, Validation Loss: 0.0361, Validation Accuracy: 86.60%\n",
      "Epoch 5/5, Training Loss: 0.3002, Validation Loss: 0.0337, Validation Accuracy: 88.07%\n",
      "Testing MLP...\n",
      "Test Accuracy: 86.70%\n",
      "\n",
      "Training CNN...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [5, 3, 3, 3], expected input[64, 5, 5, 5] to have 3 channels, but got 5 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Train CNN\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining CNN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m train(cnn_model, train_loader, valid_loader, criterion, cnn_optimizer)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Test CNN\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting CNN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 81\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     80\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 81\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     82\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     83\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 68\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m     67\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m---> 68\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     70\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [5, 3, 3, 3], expected input[64, 5, 5, 5] to have 3 channels, but got 5 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_set = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# Split training set into training and validation\n",
    "num_train = len(train_set)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.1 * num_train))  # 10% of training set for validation\n",
    "np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, sampler=train_sampler)\n",
    "valid_loader = DataLoader(train_set, batch_size=64, sampler=valid_sampler)\n",
    "\n",
    "# Define MLP model with Sigmoid activation\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define CNN model with Sigmoid activation\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 5, 3)\n",
    "        self.fc1 = nn.Linear(5 * 5 * 5, 30)\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.sigmoid(self.conv1(x)))\n",
    "        x = self.pool(torch.sigmoid(self.conv2(x)))\n",
    "        x = self.pool(torch.sigmoid(self.conv2(x)))\n",
    "        x = x.view(-1, 5 * 5 * 5)\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define training function\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                validation_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Training Loss: {running_loss/len(train_loader.dataset):.4f}, \"\n",
    "              f\"Validation Loss: {validation_loss/len(valid_loader.dataset):.4f}, \"\n",
    "              f\"Validation Accuracy: {(100 * correct / total):.2f}%\")\n",
    "\n",
    "# Define evaluation function\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Create instances of MLP and CNN\n",
    "mlp_model = MLP()\n",
    "cnn_model = CNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train MLP\n",
    "print(\"Training MLP...\")\n",
    "train(mlp_model, train_loader, valid_loader, criterion, mlp_optimizer)\n",
    "\n",
    "# Test MLP\n",
    "print(\"Testing MLP...\")\n",
    "mlp_accuracy = test(mlp_model, test_loader)\n",
    "\n",
    "# Train CNN\n",
    "print(\"\\nTraining CNN...\")\n",
    "train(cnn_model, train_loader, valid_loader, criterion, cnn_optimizer)\n",
    "\n",
    "# Test CNN\n",
    "print(\"Testing CNN...\")\n",
    "cnn_accuracy = test(cnn_model, test_loader)\n",
    "\n",
    "print(f\"\\nMLP Test Accuracy: {mlp_accuracy:.2f}%\")\n",
    "print(f\"CNN Test Accuracy: {cnn_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4248840c-439e-4e0f-b10f-cbce1377951d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP...\n",
      "Epoch 1/5, Training Loss: 0.9823, Validation Loss: 0.0594, Validation Accuracy: 82.95%\n",
      "Epoch 2/5, Training Loss: 0.8412, Validation Loss: 0.0519, Validation Accuracy: 85.67%\n",
      "Epoch 3/5, Training Loss: 0.8151, Validation Loss: 0.0512, Validation Accuracy: 86.03%\n",
      "Epoch 4/5, Training Loss: 0.7920, Validation Loss: 0.0468, Validation Accuracy: 86.05%\n",
      "Epoch 5/5, Training Loss: 0.7762, Validation Loss: 0.0463, Validation Accuracy: 87.37%\n",
      "Testing MLP...\n",
      "Test Accuracy: 86.26%\n",
      "\n",
      "Training CNN...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [5, 3, 3, 3], expected input[64, 5, 5, 5] to have 3 channels, but got 5 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 148\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Train CNN\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining CNN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 148\u001b[0m train(cnn_model, train_loader, valid_loader, criterion, cnn_optimizer)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Test CNN\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting CNN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 89\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     88\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 89\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     90\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     91\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 75\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m---> 75\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     77\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [5, 3, 3, 3], expected input[64, 5, 5, 5] to have 3 channels, but got 5 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_set = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# Split training set into training and validation\n",
    "num_train = len(train_set)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.1 * num_train))  # 10% of training set for validation\n",
    "np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, sampler=train_sampler)\n",
    "valid_loader = DataLoader(train_set, batch_size=64, sampler=valid_sampler)\n",
    "\n",
    "# Define MLP model with Dropout\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        self.dropout3 = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        return x\n",
    "\n",
    "# Define CNN model with Dropout\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 5, 3)\n",
    "        self.fc1 = nn.Linear(5 * 5 * 5, 30)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 5 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define training function\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                validation_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Training Loss: {running_loss/len(train_loader.dataset):.4f}, \"\n",
    "              f\"Validation Loss: {validation_loss/len(valid_loader.dataset):.4f}, \"\n",
    "              f\"Validation Accuracy: {(100 * correct / total):.2f}%\")\n",
    "\n",
    "# Define evaluation function\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Create instances of MLP and CNN\n",
    "mlp_model = MLP()\n",
    "cnn_model = CNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train MLP\n",
    "print(\"Training MLP...\")\n",
    "train(mlp_model, train_loader, valid_loader, criterion, mlp_optimizer)\n",
    "\n",
    "# Test MLP\n",
    "print(\"Testing MLP...\")\n",
    "mlp_accuracy = test(mlp_model, test_loader)\n",
    "\n",
    "# Train CNN\n",
    "print(\"\\nTraining CNN...\")\n",
    "train(cnn_model, train_loader, valid_loader, criterion, cnn_optimizer)\n",
    "\n",
    "# Test CNN\n",
    "print(\"Testing CNN...\")\n",
    "cnn_accuracy = test(cnn_model, test_loader)\n",
    "\n",
    "print(f\"\\nMLP Test Accuracy: {mlp_accuracy:.2f}%\")\n",
    "print(f\"CNN Test Accuracy: {cnn_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a3fab-4ad3-4a28-9119-6024af9143d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP...\n",
      "Epoch 1/50, Training Loss: 0.9823, Validation Loss: 0.0594, Validation Accuracy: 82.95%\n",
      "Epoch 2/50, Training Loss: 0.8412, Validation Loss: 0.0519, Validation Accuracy: 85.67%\n",
      "Epoch 3/50, Training Loss: 0.8151, Validation Loss: 0.0512, Validation Accuracy: 86.03%\n",
      "Epoch 4/50, Training Loss: 0.7920, Validation Loss: 0.0468, Validation Accuracy: 86.05%\n",
      "Epoch 5/50, Training Loss: 0.7762, Validation Loss: 0.0463, Validation Accuracy: 87.37%\n",
      "Epoch 6/50, Training Loss: 0.7673, Validation Loss: 0.0438, Validation Accuracy: 87.40%\n",
      "Epoch 7/50, Training Loss: 0.7485, Validation Loss: 0.0451, Validation Accuracy: 87.45%\n",
      "Epoch 8/50, Training Loss: 0.7403, Validation Loss: 0.0456, Validation Accuracy: 87.42%\n",
      "Epoch 9/50, Training Loss: 0.7434, Validation Loss: 0.0438, Validation Accuracy: 87.92%\n",
      "Epoch 10/50, Training Loss: 0.7343, Validation Loss: 0.0423, Validation Accuracy: 88.30%\n",
      "Epoch 11/50, Training Loss: 0.7198, Validation Loss: 0.0434, Validation Accuracy: 88.12%\n",
      "Epoch 12/50, Training Loss: 0.7190, Validation Loss: 0.0431, Validation Accuracy: 87.52%\n",
      "Epoch 13/50, Training Loss: 0.7109, Validation Loss: 0.0426, Validation Accuracy: 88.02%\n",
      "Epoch 14/50, Training Loss: 0.7199, Validation Loss: 0.0411, Validation Accuracy: 88.42%\n",
      "Epoch 15/50, Training Loss: 0.7079, Validation Loss: 0.0415, Validation Accuracy: 88.52%\n",
      "Epoch 16/50, Training Loss: 0.7055, Validation Loss: 0.0410, Validation Accuracy: 88.48%\n",
      "Epoch 17/50, Training Loss: 0.7001, Validation Loss: 0.0423, Validation Accuracy: 88.22%\n",
      "Epoch 18/50, Training Loss: 0.6975, Validation Loss: 0.0417, Validation Accuracy: 88.35%\n",
      "Epoch 19/50, Training Loss: 0.6991, Validation Loss: 0.0408, Validation Accuracy: 88.50%\n",
      "Epoch 20/50, Training Loss: 0.6912, Validation Loss: 0.0396, Validation Accuracy: 88.80%\n",
      "Epoch 21/50, Training Loss: 0.6862, Validation Loss: 0.0406, Validation Accuracy: 88.47%\n",
      "Epoch 22/50, Training Loss: 0.6899, Validation Loss: 0.0418, Validation Accuracy: 88.13%\n",
      "Epoch 23/50, Training Loss: 0.6826, Validation Loss: 0.0394, Validation Accuracy: 88.88%\n",
      "Epoch 24/50, Training Loss: 0.6788, Validation Loss: 0.0414, Validation Accuracy: 88.27%\n",
      "Epoch 25/50, Training Loss: 0.6802, Validation Loss: 0.0403, Validation Accuracy: 88.38%\n",
      "Epoch 26/50, Training Loss: 0.6826, Validation Loss: 0.0391, Validation Accuracy: 88.87%\n",
      "Epoch 27/50, Training Loss: 0.6723, Validation Loss: 0.0399, Validation Accuracy: 88.50%\n",
      "Epoch 28/50, Training Loss: 0.6740, Validation Loss: 0.0411, Validation Accuracy: 88.68%\n",
      "Epoch 29/50, Training Loss: 0.6663, Validation Loss: 0.0407, Validation Accuracy: 88.63%\n",
      "Epoch 30/50, Training Loss: 0.6673, Validation Loss: 0.0384, Validation Accuracy: 89.50%\n",
      "Epoch 31/50, Training Loss: 0.6644, Validation Loss: 0.0395, Validation Accuracy: 89.13%\n",
      "Epoch 32/50, Training Loss: 0.6695, Validation Loss: 0.0392, Validation Accuracy: 89.07%\n",
      "Epoch 33/50, Training Loss: 0.6638, Validation Loss: 0.0396, Validation Accuracy: 88.98%\n",
      "Epoch 34/50, Training Loss: 0.6652, Validation Loss: 0.0392, Validation Accuracy: 89.00%\n",
      "Epoch 35/50, Training Loss: 0.6563, Validation Loss: 0.0391, Validation Accuracy: 89.18%\n",
      "Epoch 36/50, Training Loss: 0.6581, Validation Loss: 0.0389, Validation Accuracy: 88.80%\n",
      "Epoch 37/50, Training Loss: 0.6578, Validation Loss: 0.0393, Validation Accuracy: 88.83%\n",
      "Epoch 38/50, Training Loss: 0.6512, Validation Loss: 0.0381, Validation Accuracy: 89.00%\n",
      "Epoch 39/50, Training Loss: 0.6520, Validation Loss: 0.0388, Validation Accuracy: 88.80%\n",
      "Epoch 40/50, Training Loss: 0.6499, Validation Loss: 0.0391, Validation Accuracy: 88.93%\n",
      "Epoch 41/50, Training Loss: 0.6451, Validation Loss: 0.0381, Validation Accuracy: 89.73%\n",
      "Epoch 42/50, Training Loss: 0.6431, Validation Loss: 0.0394, Validation Accuracy: 88.77%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_set = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# Split training set into training and validation\n",
    "num_train = len(train_set)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.1 * num_train))  # 10% of training set for validation\n",
    "np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, sampler=train_sampler)\n",
    "valid_loader = DataLoader(train_set, batch_size=64, sampler=valid_sampler)\n",
    "\n",
    "# Define MLP model with Dropout\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        self.dropout3 = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        return x\n",
    "\n",
    "# Define CNN model with Dropout\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 5, 3)\n",
    "        self.fc1 = nn.Linear(5 * 5 * 5, 30)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 5 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define training function\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, num_epochs=5):\n",
    "    train_loss_history = []\n",
    "    valid_accuracy_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                validation_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_valid_loss = validation_loss / len(valid_loader.dataset)\n",
    "        epoch_valid_accuracy = correct / total * 100\n",
    "\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "        valid_accuracy_history.append(epoch_valid_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Training Loss: {epoch_train_loss:.4f}, \"\n",
    "              f\"Validation Loss: {epoch_valid_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {epoch_valid_accuracy:.2f}%\")\n",
    "\n",
    "    return train_loss_history, valid_accuracy_history\n",
    "\n",
    "# Define evaluation function\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Create instances of MLP and CNN\n",
    "mlp_model = MLP()\n",
    "cnn_model = CNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train MLP\n",
    "print(\"Training MLP...\")\n",
    "mlp_train_loss, mlp_valid_accuracy = train(mlp_model, train_loader, valid_loader, criterion, mlp_optimizer, num_epochs=50)\n",
    "\n",
    "# Test MLP\n",
    "print(\"Testing MLP...\")\n",
    "mlp_test_accuracy = test(mlp_model, test_loader)\n",
    "\n",
    "# Train CNN\n",
    "print(\"\\nTraining CNN...\")\n",
    "cnn_train_loss, cnn_valid_accuracy = train(cnn_model, train_loader, valid_loader, criterion, cnn_optimizer, num_epochs=50)\n",
    "\n",
    "# Test CNN\n",
    "print(\"Testing CNN...\")\n",
    "cnn_test_accuracy = test(cnn_model, test_loader)\n",
    "\n",
    "print(f\"\\nMLP Test Accuracy: {mlp_test_accuracy:.2f}%\")\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy:.2f}%\")\n",
    "\n",
    "# Plot and save curves\n",
    "epochs = range(1, 51)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, mlp_train_loss, label='MLP Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('MLP Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, mlp_valid_accuracy, label='MLP Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.title('MLP Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mlp_training_curve.png')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, cnn_train_loss, label='CNN Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('CNN Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, cnn_valid_accuracy, label='CNN Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.title('CNN Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cnn_training_curve.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2041ce-f758-46dc-9ee4-cc1162ca86bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
