{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91b464c5-8a15-4ac1-85b5-a2681b8c7a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Output shape: torch.Size([1, 10])\n",
      "[1,   100] loss: 0.950\n",
      "[1,   200] loss: 0.586\n",
      "[1,   300] loss: 0.520\n",
      "[1,   400] loss: 0.498\n",
      "[1,   500] loss: 0.482\n",
      "[1,   600] loss: 0.453\n",
      "[1,   700] loss: 0.464\n",
      "[1,   800] loss: 0.447\n",
      "[2,   100] loss: 0.407\n",
      "[2,   200] loss: 0.410\n",
      "[2,   300] loss: 0.406\n",
      "[2,   400] loss: 0.398\n",
      "[2,   500] loss: 0.404\n",
      "[2,   600] loss: 0.400\n",
      "[2,   700] loss: 0.372\n",
      "[2,   800] loss: 0.388\n",
      "[3,   100] loss: 0.367\n",
      "[3,   200] loss: 0.360\n",
      "[3,   300] loss: 0.360\n",
      "[3,   400] loss: 0.359\n",
      "[3,   500] loss: 0.349\n",
      "[3,   600] loss: 0.360\n",
      "[3,   700] loss: 0.356\n",
      "[3,   800] loss: 0.361\n",
      "[4,   100] loss: 0.324\n",
      "[4,   200] loss: 0.337\n",
      "[4,   300] loss: 0.316\n",
      "[4,   400] loss: 0.336\n",
      "[4,   500] loss: 0.338\n",
      "[4,   600] loss: 0.342\n",
      "[4,   700] loss: 0.319\n",
      "[4,   800] loss: 0.325\n",
      "[5,   100] loss: 0.310\n",
      "[5,   200] loss: 0.304\n",
      "[5,   300] loss: 0.322\n",
      "[5,   400] loss: 0.312\n",
      "[5,   500] loss: 0.321\n",
      "[5,   600] loss: 0.304\n",
      "[5,   700] loss: 0.309\n",
      "[5,   800] loss: 0.301\n",
      "[6,   100] loss: 0.278\n",
      "[6,   200] loss: 0.290\n",
      "[6,   300] loss: 0.288\n",
      "[6,   400] loss: 0.312\n",
      "[6,   500] loss: 0.293\n",
      "[6,   600] loss: 0.299\n",
      "[6,   700] loss: 0.283\n",
      "[6,   800] loss: 0.299\n",
      "[7,   100] loss: 0.284\n",
      "[7,   200] loss: 0.282\n",
      "[7,   300] loss: 0.284\n",
      "[7,   400] loss: 0.296\n",
      "[7,   500] loss: 0.277\n",
      "[7,   600] loss: 0.279\n",
      "[7,   700] loss: 0.277\n",
      "[7,   800] loss: 0.264\n",
      "[8,   100] loss: 0.283\n",
      "[8,   200] loss: 0.264\n",
      "[8,   300] loss: 0.255\n",
      "[8,   400] loss: 0.276\n",
      "[8,   500] loss: 0.255\n",
      "[8,   600] loss: 0.277\n",
      "[8,   700] loss: 0.276\n",
      "[8,   800] loss: 0.271\n",
      "[9,   100] loss: 0.253\n",
      "[9,   200] loss: 0.254\n",
      "[9,   300] loss: 0.268\n",
      "[9,   400] loss: 0.256\n",
      "[9,   500] loss: 0.251\n",
      "[9,   600] loss: 0.263\n",
      "[9,   700] loss: 0.265\n",
      "[9,   800] loss: 0.270\n",
      "[10,   100] loss: 0.241\n",
      "[10,   200] loss: 0.254\n",
      "[10,   300] loss: 0.244\n",
      "[10,   400] loss: 0.253\n",
      "[10,   500] loss: 0.243\n",
      "[10,   600] loss: 0.256\n",
      "[10,   700] loss: 0.252\n",
      "[10,   800] loss: 0.243\n",
      "Finished Training\n",
      "Accuracy on the test set: 87 %\n",
      "Training with batch size 32 and learning rate 0.001\n",
      "Validation accuracy: 0.90125\n",
      "Training with batch size 32 and learning rate 0.0001\n",
      "Validation accuracy: 0.8871333333333333\n",
      "Training with batch size 32 and learning rate 1e-05\n",
      "Validation accuracy: 0.8266\n",
      "Training with batch size 64 and learning rate 0.001\n",
      "Validation accuracy: 0.9149166666666667\n",
      "Training with batch size 64 and learning rate 0.0001\n",
      "Validation accuracy: 0.8762333333333333\n",
      "Training with batch size 64 and learning rate 1e-05\n",
      "Validation accuracy: 0.8132833333333334\n",
      "Training with batch size 128 and learning rate 0.001\n",
      "Validation accuracy: 0.9130333333333334\n",
      "Training with batch size 128 and learning rate 0.0001\n",
      "Validation accuracy: 0.8663833333333333\n",
      "Training with batch size 128 and learning rate 1e-05\n",
      "Validation accuracy: 0.79235\n",
      "Best hyperparameters: {'batch_size': 64, 'lr': 0.001}\n",
      "Best validation accuracy: 0.9149166666666667\n",
      "Finished Training with best hyperparameters\n",
      "Accuracy on the test set with best hyperparameters: 87.38\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Test the model\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    model = MLP()\n",
    "    print(model)\n",
    "\n",
    "    # Create random input\n",
    "    x = torch.randn(1, 28, 28)  # Assuming input image size of 28x28\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define transformation for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define data loaders for training, validation, and test sets\n",
    "batch_size = 64\n",
    "validation_split = .1\n",
    "shuffle_dataset = True\n",
    "\n",
    "# Creating data indices for training and validation splits\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating data samplers and loaders\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the neural network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on the test set: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# Define transformation for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define data loaders for training, validation, and test sets\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rates = [0.001, 0.0001, 0.00001]\n",
    "num_epochs = 10\n",
    "\n",
    "best_accuracy = 0\n",
    "best_hyperparams = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(f'Training with batch size {batch_size} and learning rate {lr}')\n",
    "        \n",
    "        # Creating data loaders with new batch size\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Define the neural network\n",
    "        class MLP(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(MLP, self).__init__()\n",
    "                self.fc1 = nn.Linear(784, 100)\n",
    "                self.fc2 = nn.Linear(100, 50)\n",
    "                self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = x.view(-1, 784)  # Flatten the input\n",
    "                x = F.relu(self.fc1(x))\n",
    "                x = F.relu(self.fc2(x))\n",
    "                x = self.fc3(x)\n",
    "                return x\n",
    "\n",
    "        # Instantiate the model, loss function, and optimizer\n",
    "        model = MLP()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                images, labels = data\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f'Validation accuracy: {accuracy}')\n",
    "\n",
    "        # If the current hyperparameters result in higher validation accuracy, update the best hyperparameters\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_hyperparams['batch_size'] = batch_size\n",
    "            best_hyperparams['lr'] = lr\n",
    "\n",
    "print('Best hyperparameters:', best_hyperparams)\n",
    "print('Best validation accuracy:', best_accuracy)\n",
    "\n",
    "# Finally, test the model with the best hyperparameters\n",
    "best_batch_size = best_hyperparams['batch_size']\n",
    "best_lr = best_hyperparams['lr']\n",
    "\n",
    "# Creating data loaders with best hyperparameters\n",
    "best_train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "best_test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "# Define and train the model with the best hyperparameters\n",
    "best_model = MLP()\n",
    "best_optimizer = optim.Adam(best_model.parameters(), lr=best_lr)\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(best_train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        best_optimizer.zero_grad()\n",
    "\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        best_optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "print('Finished Training with best hyperparameters')\n",
    "\n",
    "# Test the model with the best hyperparameters\n",
    "best_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in best_test_loader:\n",
    "        images, labels = data\n",
    "        outputs = best_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on the test set with best hyperparameters:', 100 * correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a55a7caa-287c-4b35-913e-4bd0963bb548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch size 32 and learning rate 0.001\n",
      "Validation accuracy: 0.9162666666666667\n",
      "Training with batch size 32 and learning rate 0.0001\n",
      "Validation accuracy: 0.8828166666666667\n",
      "Training with batch size 32 and learning rate 1e-05\n",
      "Validation accuracy: 0.825\n",
      "Training with batch size 64 and learning rate 0.001\n",
      "Validation accuracy: 0.90845\n",
      "Training with batch size 64 and learning rate 0.0001\n",
      "Validation accuracy: 0.8753833333333333\n",
      "Training with batch size 64 and learning rate 1e-05\n",
      "Validation accuracy: 0.8069833333333334\n",
      "Training with batch size 128 and learning rate 0.001\n",
      "Validation accuracy: 0.9040833333333333\n",
      "Training with batch size 128 and learning rate 0.0001\n",
      "Validation accuracy: 0.8692333333333333\n",
      "Training with batch size 128 and learning rate 1e-05\n",
      "Validation accuracy: 0.7903\n",
      "Best hyperparameters: {'batch_size': 32, 'lr': 0.001}\n",
      "Best validation accuracy: 0.9162666666666667\n",
      "Finished Training with best hyperparameters\n",
      "Accuracy on the test set with best hyperparameters: 88.09\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# Define transformation for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define data loaders for training, validation, and test sets\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rates = [0.001, 0.0001, 0.00001]\n",
    "num_epochs = 10\n",
    "\n",
    "best_accuracy = 0\n",
    "best_hyperparams = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(f'Training with batch size {batch_size} and learning rate {lr}')\n",
    "        \n",
    "        # Creating data loaders with new batch size\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Define the neural network\n",
    "        class MLP(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(MLP, self).__init__()\n",
    "                self.fc1 = nn.Linear(784, 100)\n",
    "                self.fc2 = nn.Linear(100, 50)\n",
    "                self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = x.view(-1, 784)  # Flatten the input\n",
    "                x = nn.ReLU()(self.fc1(x))\n",
    "                x = nn.ReLU()(self.fc2(x))\n",
    "                x = self.fc3(x)\n",
    "                return x\n",
    "\n",
    "        # Instantiate the model, loss function, and optimizer\n",
    "        model = MLP()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                images, labels = data\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f'Validation accuracy: {accuracy}')\n",
    "\n",
    "        # If the current hyperparameters result in higher validation accuracy, update the best hyperparameters\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_hyperparams['batch_size'] = batch_size\n",
    "            best_hyperparams['lr'] = lr\n",
    "\n",
    "print('Best hyperparameters:', best_hyperparams)\n",
    "print('Best validation accuracy:', best_accuracy)\n",
    "\n",
    "# Finally, test the model with the best hyperparameters\n",
    "best_batch_size = best_hyperparams['batch_size']\n",
    "best_lr = best_hyperparams['lr']\n",
    "\n",
    "# Creating data loaders with best hyperparameters\n",
    "best_train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "best_test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "# Define and train the model with the best hyperparameters\n",
    "best_model = MLP()\n",
    "best_optimizer = optim.Adam(best_model.parameters(), lr=best_lr)\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(best_train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        best_optimizer.zero_grad()\n",
    "\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        best_optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "print('Finished Training with best hyperparameters')\n",
    "\n",
    "# Test the model with the best hyperparameters\n",
    "best_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in best_test_loader:\n",
    "        images, labels = data\n",
    "        outputs = best_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on the test set with best hyperparameters:', 100 * correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "712cd381-e568-4187-9f4d-4e8c6ceb2d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch size 32 and learning rate 0.001\n",
      "Validation accuracy: 0.9082\n",
      "Training with batch size 32 and learning rate 0.0001\n",
      "Validation accuracy: 0.8628166666666667\n",
      "Training with batch size 32 and learning rate 1e-05\n",
      "Validation accuracy: 0.73145\n",
      "Training with batch size 64 and learning rate 0.001\n",
      "Validation accuracy: 0.9107\n",
      "Training with batch size 64 and learning rate 0.0001\n",
      "Validation accuracy: 0.84935\n",
      "Training with batch size 64 and learning rate 1e-05\n",
      "Validation accuracy: 0.65485\n",
      "Training with batch size 128 and learning rate 0.001\n",
      "Validation accuracy: 0.9016666666666666\n",
      "Training with batch size 128 and learning rate 0.0001\n",
      "Validation accuracy: 0.8217\n",
      "Training with batch size 128 and learning rate 1e-05\n",
      "Validation accuracy: 0.5574\n",
      "Best hyperparameters: {'batch_size': 64, 'lr': 0.001}\n",
      "Best validation accuracy: 0.9107\n",
      "Finished Training with best hyperparameters\n",
      "Accuracy on the test set with best hyperparameters: 87.93\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# Define transformation for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define data loaders for training, validation, and test sets\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rates = [0.001, 0.0001, 0.00001]\n",
    "num_epochs = 10\n",
    "\n",
    "best_accuracy = 0\n",
    "best_hyperparams = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(f'Training with batch size {batch_size} and learning rate {lr}')\n",
    "        \n",
    "        # Creating data loaders with new batch size\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Define the neural network\n",
    "        class MLP(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(MLP, self).__init__()\n",
    "                self.fc1 = nn.Linear(784, 100)\n",
    "                self.fc2 = nn.Linear(100, 50)\n",
    "                self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = x.view(-1, 784)  # Flatten the input\n",
    "                x = nn.Sigmoid()(self.fc1(x))\n",
    "                x = nn.Sigmoid()(self.fc2(x))\n",
    "                x = self.fc3(x)\n",
    "                return x\n",
    "\n",
    "        # Instantiate the model, loss function, and optimizer\n",
    "        model = MLP()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                images, labels = data\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f'Validation accuracy: {accuracy}')\n",
    "\n",
    "        # If the current hyperparameters result in higher validation accuracy, update the best hyperparameters\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_hyperparams['batch_size'] = batch_size\n",
    "            best_hyperparams['lr'] = lr\n",
    "\n",
    "print('Best hyperparameters:', best_hyperparams)\n",
    "print('Best validation accuracy:', best_accuracy)\n",
    "\n",
    "# Finally, test the model with the best hyperparameters\n",
    "best_batch_size = best_hyperparams['batch_size']\n",
    "best_lr = best_hyperparams['lr']\n",
    "\n",
    "# Creating data loaders with best hyperparameters\n",
    "best_train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "best_test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "# Define and train the model with the best hyperparameters\n",
    "best_model = MLP()\n",
    "best_optimizer = optim.Adam(best_model.parameters(), lr=best_lr)\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(best_train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        best_optimizer.zero_grad()\n",
    "\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        best_optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "print('Finished Training with best hyperparameters')\n",
    "\n",
    "# Test the model with the best hyperparameters\n",
    "best_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in best_test_loader:\n",
    "        images, labels = data\n",
    "        outputs = best_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on the test set with best hyperparameters:', 100 * correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebef8d31-1482-4fef-89d1-4a56d4016b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch size 32 and learning rate 0.001\n",
      "Validation accuracy: 0.8969166666666667\n",
      "Training with batch size 32 and learning rate 0.0001\n",
      "Validation accuracy: 0.88435\n",
      "Training with batch size 32 and learning rate 1e-05\n",
      "Validation accuracy: 0.8104\n",
      "Training with batch size 64 and learning rate 0.001\n",
      "Validation accuracy: 0.8997833333333334\n",
      "Training with batch size 64 and learning rate 0.0001\n",
      "Validation accuracy: 0.8720666666666667\n",
      "Training with batch size 64 and learning rate 1e-05\n",
      "Validation accuracy: 0.7884833333333333\n",
      "Training with batch size 128 and learning rate 0.001\n",
      "Validation accuracy: 0.9041166666666667\n",
      "Training with batch size 128 and learning rate 0.0001\n",
      "Validation accuracy: 0.8620333333333333\n",
      "Training with batch size 128 and learning rate 1e-05\n",
      "Validation accuracy: 0.7726666666666666\n",
      "Best hyperparameters: {'batch_size': 128, 'lr': 0.001}\n",
      "Best validation accuracy: 0.9041166666666667\n",
      "Finished Training with best hyperparameters\n",
      "Accuracy on the test set with best hyperparameters: 87.69\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# Define transformation for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define data loaders for training, validation, and test sets\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rates = [0.001, 0.0001, 0.00001]\n",
    "num_epochs = 10\n",
    "\n",
    "best_accuracy = 0\n",
    "best_hyperparams = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(f'Training with batch size {batch_size} and learning rate {lr}')\n",
    "        \n",
    "        # Creating data loaders with new batch size\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Define the neural network\n",
    "        class MLP(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(MLP, self).__init__()\n",
    "                self.fc1 = nn.Linear(784, 100)\n",
    "                self.dropout1 = nn.Dropout(0.2)\n",
    "                self.fc2 = nn.Linear(100, 50)\n",
    "                self.dropout2 = nn.Dropout(0.2)\n",
    "                self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = x.view(-1, 784)  # Flatten the input\n",
    "                x = nn.ReLU()(self.fc1(x))\n",
    "                x = self.dropout1(x)\n",
    "                x = nn.ReLU()(self.fc2(x))\n",
    "                x = self.dropout2(x)\n",
    "                x = self.fc3(x)\n",
    "                return x\n",
    "\n",
    "        # Instantiate the model, loss function, and optimizer\n",
    "        model = MLP()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                images, labels = data\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f'Validation accuracy: {accuracy}')\n",
    "\n",
    "        # If the current hyperparameters result in higher validation accuracy, update the best hyperparameters\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_hyperparams['batch_size'] = batch_size\n",
    "            best_hyperparams['lr'] = lr\n",
    "\n",
    "print('Best hyperparameters:', best_hyperparams)\n",
    "print('Best validation accuracy:', best_accuracy)\n",
    "\n",
    "# Finally, test the model with the best hyperparameters\n",
    "best_batch_size = best_hyperparams['batch_size']\n",
    "best_lr = best_hyperparams['lr']\n",
    "\n",
    "# Creating data loaders with best hyperparameters\n",
    "best_train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "best_test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "# Define and train the model with the best hyperparameters\n",
    "best_model = MLP()\n",
    "best_optimizer = optim.Adam(best_model.parameters(), lr=best_lr)\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(best_train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        best_optimizer.zero_grad()\n",
    "\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        best_optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "print('Finished Training with best hyperparameters')\n",
    "\n",
    "# Test the model with the best hyperparameters\n",
    "best_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in best_test_loader:\n",
    "        images, labels = data\n",
    "        outputs = best_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on the test set with best hyperparameters:', 100 * correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d43578-fbda-4633-939e-c7ea0b72221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.2665, Validation Accuracy: 0.9082, Test Accuracy: 0.8803\n",
      "Epoch [2/10], Train Loss: 0.2533, Validation Accuracy: 0.9072, Test Accuracy: 0.8772\n",
      "Epoch [3/10], Train Loss: 0.2436, Validation Accuracy: 0.9120, Test Accuracy: 0.8798\n",
      "Epoch [4/10], Train Loss: 0.2352, Validation Accuracy: 0.9124, Test Accuracy: 0.8793\n",
      "Epoch [5/10], Train Loss: 0.2288, Validation Accuracy: 0.9208, Test Accuracy: 0.8826\n",
      "Epoch [6/10], Train Loss: 0.2204, Validation Accuracy: 0.9215, Test Accuracy: 0.8838\n",
      "Epoch [7/10], Train Loss: 0.2117, Validation Accuracy: 0.9261, Test Accuracy: 0.8846\n",
      "Epoch [8/10], Train Loss: 0.2062, Validation Accuracy: 0.9256, Test Accuracy: 0.8848\n",
      "Epoch [9/10], Train Loss: 0.1982, Validation Accuracy: 0.9278, Test Accuracy: 0.8868\n",
      "Epoch [10/10], Train Loss: 0.1949, Validation Accuracy: 0.9351, Test Accuracy: 0.8906\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define lists to store training loss, validation accuracy, and test accuracy\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(best_train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        best_optimizer.zero_grad()\n",
    "\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        best_optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            images, labels = data\n",
    "            outputs = best_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = correct / total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in best_test_loader:\n",
    "            images, labels = data\n",
    "            outputs = best_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct / total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    # Append training loss\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Validation Accuracy: {val_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Plot the curves\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Training Loss and Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('training_curves.png')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "950e089a-1cdb-4a44-a70d-9d1ddf06b1e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sum of input lengths does not equal the length of the input dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m     49\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [train_size, val_size, test_size]\n\u001b[0;32m---> 50\u001b[0m train_data, val_data, test_data \u001b[38;5;241m=\u001b[39m random_split(dataset, lengths)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# DataLoader for each set\u001b[39;00m\n\u001b[1;32m     54\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataset.py:454\u001b[0m, in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths, generator)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# Cannot verify that dataset is Sized\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset):    \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of input lengths does not equal the length of the input dataset!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    456\u001b[0m indices \u001b[38;5;241m=\u001b[39m randperm(\u001b[38;5;28msum\u001b[39m(lengths), generator\u001b[38;5;241m=\u001b[39mgenerator)\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# type: ignore[arg-type, call-overload]\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [Subset(dataset, indices[offset \u001b[38;5;241m-\u001b[39m length : offset]) \u001b[38;5;28;01mfor\u001b[39;00m offset, length \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(_accumulate(lengths), lengths)]\n",
      "\u001b[0;31mValueError\u001b[0m: Sum of input lengths does not equal the length of the input dataset!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the Convolutional Neural Network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 5, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(5, 5, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(5 * 4 * 4, 30)\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 5 * 4 * 4)  # Flatten the input for the fully connected layer\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load Fashion MNIST dataset and apply transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "dataset = FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Split dataset into training, validation, and test sets\n",
    "# Split dataset into training, validation, and test sets\n",
    "train_size = 50000\n",
    "val_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "lengths = [train_size, val_size, test_size]\n",
    "train_data, val_data, test_data = random_split(dataset, lengths)\n",
    "\n",
    "\n",
    "# DataLoader for each set\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluating the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy on test set: {100 * accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c49d51-21e3-4104-8340-ff9d93f28f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
